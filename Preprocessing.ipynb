{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFO-H-515 Project <br>\n",
    "2022–2023\n",
    "\n",
    "# Phase 2 : Preprocessing\n",
    "Gianluca Bontempi, Théo Verhelst, Cédric Simar <br>\n",
    "Computer Science Department, ULB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "The idea is to create the perfect table in python and then turn it into a spark dataframe. As such, for each sensor, we can merge the perfect table with "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, rank\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[10]\")\\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.executor.cores\", \"10\") \\\n",
    "    .config(\"spark.executor.memory\", \"16G\") \\\n",
    "    .appName(\"¨Preprocessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Let us retrieve the sparkContext object\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is running the script `getdata.py` which is querying the data concurrently. It first get the sensor from the API, then it loads the sensors, put them into an RDD and parallelizing the query of the results. NB : there is a known bug for users on Mac OS with M-chips. The latter doesn't allow slaves from spark to make external requests. In the docker image, the function run fine, except two warnings in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python getdata.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the IDs in the csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to add to each file the sensor_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "PATH = \"data/sensors\"\n",
    "OUT_dir = \"data/processed\"\n",
    "\n",
    "if not os.path.exists(OUT_dir):\n",
    "    os.makedirs(OUT_dir)\n",
    "\n",
    "files = [os.path.join(PATH, f) for f in os.listdir(PATH)]\n",
    "\n",
    "def add_sensor_information(filename, output_dir) -> None:\n",
    "    sensor_name = filename.split('.')[0].split(\"_\")[-1]\n",
    "    sensor_data = pd.read_csv(filename)\n",
    "    sensor_data[\"sensor\"] = sensor_name\n",
    "    sensor_data.to_csv(f'{output_dir}/{sensor_name}.csv', index=False)\n",
    "\n",
    "parallel_files = sc.parallelize(files)\n",
    "\n",
    "parallel_files.foreach(lambda x : add_sensor_information(x, OUT_dir))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesing the missing timestamps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discrepencies in the format for missing data can be solved by creating a perfect table, on which the real data will be joined. Missing records will be then just filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join(OUT_dir, f) for f in os.listdir(OUT_dir)]\n",
    "sensors = [f.split(\".\")[0].split(\"/\")[-1] for f in files]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the perfect table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_days(start_date: datetime.date, end_date: datetime.date) -> list[str]:\n",
    "    \"\"\"\n",
    "    Generates a list of dates between two dates.\n",
    "\n",
    "    Args:\n",
    "        start_date (datetime.date): The start date of the range.\n",
    "        end_date (datetime.date): The end date of the range.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of dates between the start and end dates.\n",
    "    \"\"\"\n",
    "    days = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        days.append(current_date.strftime(\"%Y-%m-%d\"))\n",
    "        current_date += datetime.timedelta(days=1)\n",
    "    return days\n",
    "\n",
    "intervals = 96\n",
    "\n",
    "# Generate a list of dates between the specified start and end dates\n",
    "days = generate_days(datetime.date(2018, 12, 6), datetime.date(2023, 3, 31))\n",
    "\n",
    "# Create a perfect frame by combining each date with a range of intervals\n",
    "perfect_frame = [[day, i] for day in days for i in range(1, intervals+1)]\n",
    "\n",
    "def add_sensor(x: list, sensors: list) -> list:\n",
    "    \"\"\"\n",
    "    Adds sensor information to the input list.\n",
    "\n",
    "    Args:\n",
    "        x (list): The input list to which sensor information will be added.\n",
    "        sensors (list): The list of sensors to add.\n",
    "\n",
    "    Returns:\n",
    "        list: The input list with sensor information added.\n",
    "    \"\"\"\n",
    "    return [x + [sensor] for sensor in sensors]\n",
    "\n",
    "# Create an RDD from the perfect_frame with only the dates to add the sensors\n",
    "rdd_pf = sc.parallelize(perfect_frame)\\\n",
    "    .flatMap(lambda x: add_sensor(x, sensors))\\\n",
    "    .map(lambda x: (x[0]+ \"-\" + str(x[1]) + \"-\" + str(x[2]), x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding missing timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_files = sc.textFile(OUT_dir)\\\n",
    "    .map(lambda x: x.split(','))\\\n",
    "    .filter(lambda x: x[0] != \"Date\")\\\n",
    "    .map(lambda x: (x[0]+ \"-\" + str(x[1]) + \"-\" + str(x[-1]), x))\n",
    "\n",
    "def validate_results(x):\n",
    "    \"\"\"\n",
    "    Validates the results by checking if the join could have been made.\n",
    "    In the other case, there was an empty record, and we can fill it.\n",
    "\n",
    "    Args:\n",
    "        x: The input tuple containing the key-value pair.\n",
    "\n",
    "    Returns:\n",
    "        list: The validated results.\n",
    "    \"\"\"\n",
    "    if x[1] == None:\n",
    "        return [x[0][0], int(x[0][1]), 0, -1] + x[0][2:]\n",
    "    else :\n",
    "        x[1][1] = int(x[1][1])\n",
    "        x[1][2] = int(x[1][2])\n",
    "        x[1][3] = int(x[1][3])\n",
    "        return x[1]\n",
    "    \n",
    "processed_files = rdd_pf.leftOuterJoin(raw_files)\\\n",
    "                    .mapValues(validate_results)\\\n",
    "                    .map(lambda x: x[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding time index\n",
    "\n",
    "This extra step is to add a unique timestamp that is used as counter to ease the computation later on. This is step is not necessary, this column could be added while loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "columns = ['Date', 'Time Gap', 'Count', 'Average speed', 'sensor']\n",
    "processed_files_df = spark.createDataFrame(processed_files,columns)\\\n",
    "                                    .sort(\"Date\", \"Time Gap\", \"sensor\")\n",
    "\n",
    "w = Window().partitionBy(\"sensor\").orderBy(\"Date\", \"Time Gap\")\n",
    "processed_files_df = processed_files_df.withColumn('timestamp',rank().over(w))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def move_csv_files(directory: str, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Moves all CSV files from the specified directory to a single file with the given filename.\n",
    "    Deletes the original directory after moving the files.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The directory path where the CSV files are located.\n",
    "        filename (str): The name of the file to which the CSV files will be moved.\n",
    "    \"\"\"\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".csv\"):\n",
    "            # Rename each CSV file to the specified filename\n",
    "            os.rename(os.path.join(directory, file), os.path.join(directory, filename))\n",
    "\n",
    "            # Move the renamed file to the current directory\n",
    "            shutil.move(os.path.join(directory, filename), \".\")\n",
    "\n",
    "            # Remove the original directory after moving the file\n",
    "            shutil.rmtree(directory)\n",
    "\n",
    "    return None\n",
    "\n",
    "folder = \"bike_counts\"\n",
    "\n",
    "# Write the processed_files_df DataFrame as a single CSV file in the specified folder\n",
    "processed_files_df.coalesce(1).write.options(header='True', delimiter=',').csv(f\"{folder}\")\n",
    "\n",
    "# Move the CSV file(s) in the folder to a single file named \"bike_counts.csv\"\n",
    "move_csv_files(f\"{folder}\", \"bike_counts.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Date\", DateType(), nullable = False),\n",
    "    StructField(\"Time Gap\", IntegerType(), nullable = False),\n",
    "    StructField(\"Count\", IntegerType(), nullable = False),\n",
    "    StructField(\"Average speed\", IntegerType(), nullable = False),\n",
    "    StructField(\"sensor\", StringType(), nullable = False),\n",
    "    StructField(\"timestamp\", IntegerType(), nullable = False)\n",
    "    ]) \n",
    "\n",
    "bike_counts = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", True) \\\n",
    "        .schema(schema) \\\n",
    "        .load(\"bike_counts.csv\")\\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+-------------+-------+---------+\n",
      "|      Date|Time Gap|Count|Average speed| sensor|timestamp|\n",
      "+----------+--------+-----+-------------+-------+---------+\n",
      "|2018-12-06|       1|    0|           -1|CB02411|        1|\n",
      "|2018-12-06|       2|    0|           -1|CB02411|        2|\n",
      "|2018-12-06|       3|    0|           -1|CB02411|        3|\n",
      "|2018-12-06|       4|    0|           -1|CB02411|        4|\n",
      "|2018-12-06|       5|    0|           -1|CB02411|        5|\n",
      "|2018-12-06|       6|    0|           -1|CB02411|        6|\n",
      "|2018-12-06|       7|    0|           -1|CB02411|        7|\n",
      "|2018-12-06|       8|    0|           -1|CB02411|        8|\n",
      "|2018-12-06|       9|    0|           -1|CB02411|        9|\n",
      "|2018-12-06|      10|    0|           -1|CB02411|       10|\n",
      "|2018-12-06|      11|    0|           -1|CB02411|       11|\n",
      "|2018-12-06|      12|    0|           -1|CB02411|       12|\n",
      "|2018-12-06|      13|    0|           -1|CB02411|       13|\n",
      "|2018-12-06|      14|    0|           -1|CB02411|       14|\n",
      "|2018-12-06|      15|    0|           -1|CB02411|       15|\n",
      "|2018-12-06|      16|    0|           -1|CB02411|       16|\n",
      "|2018-12-06|      17|    0|           -1|CB02411|       17|\n",
      "|2018-12-06|      18|    0|           -1|CB02411|       18|\n",
      "|2018-12-06|      19|    0|           -1|CB02411|       19|\n",
      "|2018-12-06|      20|    0|           -1|CB02411|       20|\n",
      "+----------+--------+-----+-------------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bike_counts.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly 18 records per Date per time stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:===========>                                             (2 + 8) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+\n",
      "|Date|Time Gap|count|\n",
      "+----+--------+-----+\n",
      "+----+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bike_counts.groupBy(\"Date\", \"Time Gap\").count().where((col(\"count\") != 18) ).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of record per day for a sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+\n",
      "|sensor|Date|count|\n",
      "+------+----+-----+\n",
      "+------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bike_counts.groupBy(\"sensor\", \"Date\").count().where((col(\"count\") != 96)).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Closing the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
