{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFO-H-515 Project <br>\n",
    "2022–2023\n",
    "\n",
    "# Phase 2 : Producer\n",
    "Gianluca Bontempi, Théo Verhelst, Cédric Simar <br>\n",
    "Computer Science Department, ULB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information\n",
    "Group Number : 5 <br>\n",
    "Group Members : Rania Baguia (000459242), Hakim Amri (000459153), Julian Cailliau (000459856), Mehdi Jdaoudi (000457507)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, rank, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "import socket\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key notebook variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH:str = \"bike_counts.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producer\n",
    "We opted for the socket based approach for sending data, instead of the file based system. As such, the producer would first bind the port 9999 on the localhost. Then we create a producer spark session so that it can read the csv table as a spark dataframe and benefiting from parallelisation in reading the file. Finally, data is being query to the dataframe, collected, turned into an array, encoded and sent over the socket."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring the producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the server name and port name\n",
    "host = 'localhost'\n",
    "port = 9999\n",
    "  \n",
    "# create a socket at server side\n",
    "# using TCP / IP protocol\n",
    "s = socket.socket(socket.AF_INET,\n",
    "                  socket.SOCK_STREAM)\n",
    "  \n",
    "# bind the socket with server\n",
    "# and port number\n",
    "s.bind((host, port))\n",
    "  \n",
    "s.listen(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the spark session for the producer\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[10]\")\\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.executor.cores\", \"10\") \\\n",
    "    .config(\"spark.executor.memory\", \"16G\") \\\n",
    "    .appName(\"Producer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Let us retrieve the sparkContext object\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the necessary information\n",
    "We first load the sensors information, essentially sensor names. This allows us to restrict the producer to send data related to a predifined number of sensor. Such processing is done for the scalability purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the file with the sensors information. As the latter is small, it can be read using simply python\n",
    "sensors = []\n",
    "with open(\"data/bikes_sensors.json\", \"r\") as f:\n",
    "    sensors = json.load(f)\n",
    "    sensors = [\n",
    "        sensor[\"properties\"][\"device_name\"] for sensor in sensors[\"features\"]\n",
    "    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can use spark to read the csv, by first giving the schema of the data. It is worth noting that the date is being typed as a `StringType()`. The reason is that, reading it as `DateType()` was causing issues in the formating as it inserting `,` in the records. Thus when the consummer needs to evaluate the input shape, it had issues understanding the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Date\", StringType(), nullable = False),\n",
    "    StructField(\"Time Gap\", IntegerType(), nullable = False),\n",
    "    StructField(\"Count\", IntegerType(), nullable = False),\n",
    "    StructField(\"Average speed\", IntegerType(), nullable = False),\n",
    "    StructField(\"sensor\", StringType(), nullable = False),\n",
    "    StructField(\"timestamp\", IntegerType(), nullable = False)\n",
    "    ]) \n",
    "\n",
    "bike_counts = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", True) \\\n",
    "        .schema(schema) \\\n",
    "        .load(FILE_PATH)\\\n",
    "        .orderBy(\"timestamp\", \"sensor\")\\\n",
    "        .cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the producer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To send the data over sockets, three parameters are key :\n",
    "- `batchTimeInterval` which is the time interval (sec.) between two batches.\n",
    "- `timePeriod` which is the number of days to send in a batch\n",
    "- `n_sensors` which is a variable indicating the number of sensors to consider (for scalability essentially). If set to `-1`, then all the sensors are considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 # Counter variable\n",
    "batchTimeInterval = 10 # Time interval between batches\n",
    "timePeriod = 10 # Number of days to send in a batch\n",
    "timeGaps = 96 # Number of time gaps in a day\n",
    "n_sensors = -1 # Number of sensors (-1 indicates no restriction)\n",
    "\n",
    "# Checking if the number of sensors is valid\n",
    "if n_sensors != -1 :\n",
    "    if n_sensors > len(sensors) :\n",
    "        # Warning message if the requested number of sensors is larger than the available number\n",
    "        logging.warning(f\"The number of sensors required {n_sensors} is larger than the actual amount of sensors available {len(sensors)}. Truncating the number of sensors to {len(sensors)}.\")\n",
    "        n_sensors = len(sensors)\n",
    "    # Selecting a subset of sensors based on the requested number\n",
    "    sensors_restricted = sensors[:n_sensors]\n",
    "\n",
    "timestepsPerBatch = timePeriod * timeGaps # Number of timesteps per batch per sensor\n",
    "\n",
    "logging.info(f\"Waiting for connections on port {port}\")\n",
    "c, addr = s.accept()\n",
    "logging.info(f\"Connection from : {str(addr)}\")\n",
    "\n",
    "while True:\n",
    "    LB = i * timestepsPerBatch # Lower bound of timestamp range\n",
    "    HB = i * timestepsPerBatch + timestepsPerBatch + 1 # Upper bound of timestamp range\n",
    "    if n_sensors == -1 :\n",
    "        # Filtering bike_counts based on timestamp range only \n",
    "        query = bike_counts.filter((col(\"timestamp\") > LB) & (col(\"timestamp\") < HB))\n",
    "    else :\n",
    "        # Filtering bike_counts based on timestamp range and restricted sensors\n",
    "        query = bike_counts.filter((col(\"timestamp\") > LB) & (col(\"timestamp\") < HB) & (col(\"sensor\").isin(sensors_restricted)))\n",
    "\n",
    "    arr = np.array(query.collect()) # Collecting query results as a numpy array\n",
    "    if arr.shape[0] > 0 : \n",
    "        #  Converting numpy array to string and sending it over the connection\n",
    "        message  = np.array2string(arr, separator=\",\", threshold=np.inf).replace(\"\\n\", \"\").replace(\" \", \"\") + \"\\n\"\n",
    "        try:  \n",
    "            c.send(message.encode())\n",
    "        except socket.error:\n",
    "            c.close()\n",
    "            c, addr = s.accept()\n",
    "    else :\n",
    "        # Logging a message when all the data has been consumed and closing the connection \n",
    "        logging.info(f\"Consummed all the data.\")\n",
    "        c.close()\n",
    "        break\n",
    "    \n",
    "    time.sleep(batchTimeInterval) # Waiting for the specified time interval\n",
    "    i  += 1 # Incrementing the counter for the next batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
