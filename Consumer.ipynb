{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFO-H-515 Project <br>\n",
    "2022–2023\n",
    "\n",
    "# Phase 2 : Consumer\n",
    "Gianluca Bontempi, Théo Verhelst, Cédric Simar <br>\n",
    "Computer Science Department, ULB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information\n",
    "Group Number : 5 <br>\n",
    "Group Members : Rania Baguia (000459242), Hakim Amri (000459153), Julian Cailliau (000459856), Mehdi Jdaoudi (000457507)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, rank, monotonically_increasing_id\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.streaming import StreamingContext\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re, ast\n",
    "import socket\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[10]\")\\\n",
    "    .config(\"spark.executor.instances\", \"1\") \\\n",
    "    .config(\"spark.executor.cores\", \"10\") \\\n",
    "    .config(\"spark.executor.memory\", \"16G\") \\\n",
    "    .appName(\"Consummer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Let us retrieve the sparkContext object\n",
    "sc=spark.sparkContext\n",
    "\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "logger = spark._jvm.org.apache.log4j\n",
    "logging.getLogger(\"py4j.java_gateway\").setLevel(logging.ERROR)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the necessary information\n",
    "We first load the sensors information, essentially sensor names. This allows us to programmatically create states for the following models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = []\n",
    "with open(\"data/bikes_sensors.json\", \"r\") as f:\n",
    "    sensors = json.load(f)\n",
    "    sensors = [\n",
    "        sensor[\"properties\"][\"device_name\"] for sensor in sensors[\"features\"]\n",
    "    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the Stream function\n",
    "This function will create a streaming context from a network socket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDStream(sc, batch_interval):\n",
    "    \"\"\"\n",
    "    Create a streaming context and a DStream from a network socket.\n",
    "\n",
    "    Args:\n",
    "        sc (SparkContext): The Spark context object.\n",
    "        batch_interval (int): The time interval in seconds at which streaming data will be divided into batches.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the streaming context (ssc) and the DStream (dstream).\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "\n",
    "    Example:\n",
    "        >>> sc = SparkContext(appName=\"StreamingExample\")\n",
    "        >>> ssc, dstream = getDStream(sc, 5)\n",
    "    \"\"\"\n",
    "\n",
    "    #Create streaming context, with required batch interval\n",
    "    ssc = StreamingContext(sc, batch_interval)\n",
    "\n",
    "    #Checkpointing needed for stateful transforms\n",
    "    ssc.checkpoint(\"checkpoint\")\n",
    "    \n",
    "    # Create a DStream that represents streaming data from a network socket\n",
    "    dstream = ssc.socketTextStream(\"localhost\", 9999)\n",
    "    \n",
    "    return [ssc,dstream]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key notebook variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NSENSORS = 18"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Persistance Model\n",
    "A simple naive model returning the last observed value per sensor running concurrently. The equation is like the following :\n",
    "\\begin{equation*}\n",
    "\\hat{y}(t+1, sensor) = y(t, sensor)\n",
    "\\end{equation*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to define the state of such a model. This is done programmatically for every available sensor. It is worth noting that, the producer have the ability to send only few sensors. Then, on the consummer side, all the sensors will be available in the states, however, only the one sent will be updated. Those can be differentiated with the `N` state which is the number of forecast. They will have 0 forecasts. It is also worth noting that each sensor have its own record in the RDD with its name as key. This allows for parallel updates of the states across sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PersModelMSE = [(i,{\"SSE\":0, \"Forecast\" : 0, \"N\":0, \"N_MSE\":0 ,\"MSE\":0}) for i in sensors]\n",
    "PersModelRDD = sc.parallelize(PersModelMSE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to create the update function for updating the states. We went one step ahead by wrapping the function into a function that is returning the update function. This pattern allows us to pass custom arguments to update function that is passed to `.updateStateByKey()`. As such, we designed the function to be able to compute the MSE through the whole period, or between a specific time interval (which is usefull for the question 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createUpdateFunction(MSE_range:tuple=(None, None),)  -> callable:\n",
    "    \"\"\"\n",
    "    Create an update function for stateful streaming calculations.\n",
    "\n",
    "    Args:\n",
    "        MSE_range (tuple, optional): A tuple representing the range of \n",
    "        MSE (Mean Squared Error) dates in the format (\"%Y-%m-%d\", \"%Y-%m-%d\"). During this range,\n",
    "        the MSE will be computed, considering the past as solely training. \n",
    "        Defaults to (None, None).\n",
    "\n",
    "    Returns:\n",
    "        function: The update function that calculates and updates the state based on new values.\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "\n",
    "    Example:\n",
    "        >>> update_func = createUpdateFunction((\"2022-01-01\", \"2022-12-31\"))\n",
    "    \"\"\"\n",
    "    if MSE_range != (None, None):\n",
    "        LB = datetime.datetime.strptime(MSE_range[0], \"%Y-%m-%d\")\n",
    "        HB = datetime.datetime.strptime(MSE_range[1], \"%Y-%m-%d\")\n",
    "\n",
    "    def updateFunction(new_values, state) -> dict:\n",
    "        \"\"\"\n",
    "        Update function that calculates and updates the state based on new values.\n",
    "\n",
    "        Args:\n",
    "            new_values (list): A list of new values.\n",
    "            state (dict): The current state.\n",
    "\n",
    "        Returns:\n",
    "            dict: The updated state.\n",
    "\n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\" \n",
    "        L=len(new_values) \n",
    "        if (L>0):\n",
    "            initial_state = state\n",
    "            Observations = sorted(new_values[0], key = lambda x: x[5], reverse=False)\n",
    "            ObservationsCounts = [i[2] for i in Observations]\n",
    "            for i in range(0, len(ObservationsCounts)):\n",
    "                if initial_state[\"N\"] != 0 :\n",
    "                    if MSE_range == (None, None):\n",
    "                        err = ObservationsCounts[i] - initial_state[\"Forecast\"] \n",
    "                        initial_state[\"SSE\"] = initial_state[\"SSE\"] + err ** 2\n",
    "                        initial_state[\"MSE\"] = initial_state[\"SSE\"]/initial_state[\"N\"]\n",
    "                    else :\n",
    "                        if LB <= datetime.datetime.strptime(Observations[i][0], \"%Y-%m-%d\") <= HB:\n",
    "                            if initial_state[\"N_MSE\"] != 0 :\n",
    "                                err = ObservationsCounts[i] - initial_state[\"Forecast\"] \n",
    "                                initial_state[\"SSE\"] = (initial_state[\"SSE\"] if initial_state[\"SSE\"] != -np.inf else 0) + err ** 2\n",
    "                                initial_state[\"MSE\"] = initial_state[\"SSE\"]/initial_state[\"N_MSE\"]\n",
    "                            initial_state[\"N_MSE\"] = initial_state[\"N_MSE\"] + 1\n",
    "                        else :\n",
    "                            initial_state[\"SSE\"] = -np.inf\n",
    "                            initial_state[\"MSE\"] = -np.inf\n",
    "                initial_state[\"Forecast\"] =  ObservationsCounts[i]\n",
    "                initial_state[\"N\"] = initial_state[\"N\"] + 1  \n",
    "                \n",
    "            return initial_state   \n",
    "        else:\n",
    "            return state\n",
    "        \n",
    "    return updateFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_INTERVAL = 10\n",
    "# Get the DStream (discretized stream) using the specified batch interval\n",
    "[ssc,dstream]=getDStream(sc, BATCH_INTERVAL)\n",
    "\n",
    "# Flatten the input stream and convert each element to a tuple\n",
    "dataS = dstream\\\n",
    "    .flatMap(lambda x: [*np.array(ast.literal_eval(x))])\\\n",
    "    .map(lambda x: (x[0], int(x[1]), int(x[2]), int(x[3]), x[4], int(x[5])))\\\n",
    "\n",
    "# Map each tuple by the sensor ID and partition the data by the number of sensors\n",
    "dataPerSensor = dataS\\\n",
    "    .map(lambda x: (x[4], x))\\\n",
    "    .partitionBy(NSENSORS)\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(list)\n",
    "\n",
    "# Create an update state function using the specified MSE range\n",
    "updateStateFunction = createUpdateFunction(MSE_range=(\"2022-04-15\", \"2023-03-31\"))\n",
    "\n",
    "# Update the state of the DStream using the update state function, with an initial RDD provided\n",
    "FutureForecasts = dataPerSensor\\\n",
    "    .updateStateByKey(updateStateFunction, initialRDD=PersModelRDD)\n",
    "\n",
    "# Print the future forecasts to the console\n",
    "FutureForecasts.pprint()\n",
    "\n",
    "# Save each RDD in the DStream as a text file\n",
    "FutureForecasts.foreachRDD(lambda rdd: rdd.saveAsTextFile(\"FP_final_state_.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Persistence Model\n",
    "A weighted persistence model returning the average of the last V values per sensor running concurrently with V = 2, 3, 4. The equation is like the following :\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{y}(t+1, sensor, V) = \\dfrac{\\sum_{n=0}^{V-1} y(t-n, sensor)}{V}\n",
    "\\end{equation*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The createUpdateFunction is very similar to the simple persistance model, except we add the ability to have an average window for the forecast. This is relevant for changing the parameter and evaluating the model again. Furthermore, a trick we used for evaluating the borders the batch, we just store the last observation for the average computation in the state, and then we add the observation at the begining of the upcomming batch. This allows us to simply \"roll\" the average computaion and not miss borders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createUpdateFunction(average_window:int, MSE_range:tuple=(None, None)) -> callable:\n",
    "    \"\"\"\n",
    "    Create an update function for stateful stream processing.\n",
    "\n",
    "    Args:\n",
    "        average_window (int): The size of the moving average window for forecasting.\n",
    "        MSE_range (tuple, optional): A tuple representing the range of \n",
    "        MSE (Mean Squared Error) dates in the format (\"%Y-%m-%d\", \"%Y-%m-%d\"). During this range,\n",
    "        the MSE will be computed, considering the past as solely training.  \n",
    "        Defaults to (None, None).\n",
    "\n",
    "    Returns:\n",
    "        function: The update function that calculates and updates the state based on new values.\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "\n",
    "    Example:\n",
    "        >>> update_func = createUpdateFunction(5, (\"2022-01-01\", \"2022-12-31\"))\n",
    "    \"\"\"\n",
    "    if MSE_range != (None, None):\n",
    "        LB = datetime.datetime.strptime(MSE_range[0], \"%Y-%m-%d\")\n",
    "        HB = datetime.datetime.strptime(MSE_range[1], \"%Y-%m-%d\")\n",
    "\n",
    "    def updateFunction(new_values, state) -> dict: \n",
    "        L=len(new_values) \n",
    "        if (L>0):\n",
    "            initial_state = state\n",
    "            Observations = sorted(initial_state[\"LastObservations\"] + list(new_values[0]), key = lambda x: x[5], reverse=False)\n",
    "            ObservationsCounts = [i[2] for i in Observations]\n",
    "\n",
    "            for i in range(average_window-1, len(ObservationsCounts)):\n",
    "                if initial_state[\"N\"] != 0 :\n",
    "                    if MSE_range == (None, None):\n",
    "                        err = ObservationsCounts[i] - initial_state[\"Forecast\"] \n",
    "                        initial_state[\"SSE\"] = initial_state[\"SSE\"] + err ** 2\n",
    "                        initial_state[\"MSE\"] = initial_state[\"SSE\"]/initial_state[\"N\"]\n",
    "                    else :\n",
    "                        if LB <= datetime.datetime.strptime(Observations[i][0], \"%Y-%m-%d\") <= HB:\n",
    "                            if initial_state[\"N_MSE\"] != 0 :\n",
    "                                err = ObservationsCounts[i] - initial_state[\"Forecast\"] \n",
    "                                initial_state[\"SSE\"] = (initial_state[\"SSE\"] if initial_state[\"SSE\"] != -np.inf else 0) + err ** 2\n",
    "                                initial_state[\"MSE\"] = initial_state[\"SSE\"]/initial_state[\"N_MSE\"]\n",
    "                            initial_state[\"N_MSE\"] = initial_state[\"N_MSE\"] + 1\n",
    "                        else :\n",
    "                            initial_state[\"SSE\"] = -np.inf\n",
    "                            initial_state[\"MSE\"] = -np.inf\n",
    "                \n",
    "                initial_state[\"Forecast\"] =  np.mean(np.array(ObservationsCounts[i-average_window+1:i+1]))\n",
    "                initial_state[\"N\"] = initial_state[\"N\"] + 1\n",
    "\n",
    "            initial_state[\"LastObservations\"] = Observations[-average_window + 1:]\n",
    "                \n",
    "            return initial_state   \n",
    "        else:\n",
    "            return state\n",
    "        \n",
    "    return updateFunction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Order 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can just set a parameter `WEIGHTED_AVG_LEN` which can be passed to the `createUpdateFunction`. The stream processing is similar to the begining. The only difference is the state to save, which adds the previous observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTED_AVG_LEN = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PersModelAverage = [(i,{\"SSE\":0, \"Forecast\" : 0, \"N\":0, \"N_MSE\":0 ,\"MSE\":0, \"LastObservations\":[]}) for i in sensors]\n",
    "PersModelAverageRDD = sc.parallelize(PersModelAverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ssc,dstream]=getDStream(sc, BATCH_INTERVAL)\n",
    "\n",
    "dataS = dstream\\\n",
    "    .flatMap(lambda x: [*np.array(ast.literal_eval(x))])\\\n",
    "    .map(lambda x: (x[0], int(x[1]), int(x[2]), int(x[3]), x[4], int(x[5])))\\\n",
    "\n",
    "dataPerSensor = dataS\\\n",
    "    .map(lambda x: (x[4], x))\\\n",
    "    .partitionBy(NSENSORS)\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(list)\n",
    "    \n",
    "updateStateFunction = createUpdateFunction(average_window = WEIGHTED_AVG_LEN, MSE_range=(\"2022-04-15\", \"2023-03-31\"))\n",
    "\n",
    "FutureForecasts = dataPerSensor\\\n",
    "    .updateStateByKey(updateStateFunction, initialRDD=PersModelAverageRDD)\n",
    "\n",
    "FutureForecasts.pprint()\n",
    "\n",
    "FutureForecasts.foreachRDD(lambda rdd: rdd.saveAsTextFile(f\"WP_final_state_{WEIGHTED_AVG_LEN}.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Order 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTED_AVG_LEN = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PersModelAverage = [(i,{\"SSE\":0, \"Forecast\" : 0, \"N\":0, \"N_MSE\":0 ,\"MSE\":0, \"LastObservations\":[]}) for i in sensors]\n",
    "PersModelAverageRDD = sc.parallelize(PersModelAverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ssc,dstream]=getDStream(sc, BATCH_INTERVAL)\n",
    "\n",
    "dataS = dstream\\\n",
    "    .flatMap(lambda x: [*np.array(ast.literal_eval(x))])\\\n",
    "    .map(lambda x: (x[0], int(x[1]), int(x[2]), int(x[3]), x[4], int(x[5])))\\\n",
    "\n",
    "dataPerSensor = dataS\\\n",
    "    .map(lambda x: (x[4], x))\\\n",
    "    .partitionBy(NSENSORS)\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(list)\n",
    "    \n",
    "updateStateFunction = createUpdateFunction(average_window = WEIGHTED_AVG_LEN, MSE_range=(\"2022-04-15\", \"2023-03-31\"))\n",
    "\n",
    "FutureForecasts = dataPerSensor\\\n",
    "    .updateStateByKey(updateStateFunction, initialRDD=PersModelAverageRDD)\n",
    "\n",
    "FutureForecasts.pprint()\n",
    "\n",
    "FutureForecasts.foreachRDD(lambda rdd: rdd.saveAsTextFile(f\"WP_final_state_{WEIGHTED_AVG_LEN}.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Order 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTED_AVG_LEN = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PersModelAverage = [(i,{\"SSE\":0, \"Forecast\" : 0, \"N\":0, \"N_MSE\":0 ,\"MSE\":0, \"LastObservations\":[]}) for i in sensors]\n",
    "PersModelAverageRDD = sc.parallelize(PersModelAverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ssc,dstream]=getDStream(sc, BATCH_INTERVAL)\n",
    "\n",
    "dataS = dstream\\\n",
    "    .flatMap(lambda x: [*np.array(ast.literal_eval(x))])\\\n",
    "    .map(lambda x: (x[0], int(x[1]), int(x[2]), int(x[3]), x[4], int(x[5])))\\\n",
    "\n",
    "dataPerSensor = dataS\\\n",
    "    .map(lambda x: (x[4], x))\\\n",
    "    .partitionBy(NSENSORS)\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(list)\n",
    "    \n",
    "updateStateFunction = createUpdateFunction(average_window = WEIGHTED_AVG_LEN, MSE_range=(\"2022-04-15\", \"2023-03-31\"))\n",
    "\n",
    "FutureForecasts = dataPerSensor\\\n",
    "    .updateStateByKey(updateStateFunction, initialRDD=PersModelAverageRDD)\n",
    "\n",
    "FutureForecasts.pprint()\n",
    "\n",
    "FutureForecasts.foreachRDD(lambda rdd: rdd.saveAsTextFile(f\"WP_final_state_{WEIGHTED_AVG_LEN}.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Least Squares\n",
    "The recursive least squares is an online learning method. It can learn while observing passing data to predict following observation in a regression setting.  \n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{cases}\n",
    "V_{(t)}&=\\frac{1}{\\nu} \\left(V_{(t-1)}\n",
    "-\\frac{V_{(t-1)} x^T_{t} x_{t} V_{(t-1)}}{1+ x_{t} V_{(t-1)} x^T_{t}} \\right)\\\\[3pt]\n",
    "\\alpha_{(t)}&= V_{(t)} x^T_{t} \\\\[3pt]\n",
    "e&= y_{t}- x_{t} \\hat{\\beta}_{(t-1)}  \\\\[3pt]\n",
    "\\hat{\\beta}_{(t)}&=\\hat{\\beta}_{(t-1)}+ \\alpha_{(t)} e \\\\[3pt]\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "where $V$ is the covariance matrix and $\\beta$ is the set of parameters of the linear model.\n",
    "\n",
    "We first tried this approach using only the last $n$ observation (embedding order). Then we added numerical features to the observation so it can better fine tune the prediction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedded RLS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is very similar to the weighted average. The difference is the forecasting strategy. In fact, we first initialize the parameters (beta, V and the forgetting factor) per sensor. Then we can use those values and make a forecast. After the first step, we can load those values, correct them with the forecasting error, and then making a forecast for the next data and update the states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RLSstep(y: np.ndarray, yhat: np.ndarray, x: np.ndarray, Beta: np.ndarray, Var: np.ndarray, nu: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Performs a Recursive Least Squares (RLS) step for parameter estimation.\n",
    "\n",
    "    Args:\n",
    "        y (np.ndarray): The observed output values.\n",
    "        yhat (np.ndarray): The predicted output values.\n",
    "        x (np.ndarray): The input feature values.\n",
    "        Beta (np.ndarray): The estimated model parameters.\n",
    "        Var (np.ndarray): The estimated parameter covariance matrix.\n",
    "        nu (float): A forgetting factor.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing updated values of yhat (predicted output), err (prediction error),\n",
    "               Var (updated parameter covariance matrix), and Beta (updated model parameters).\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "\n",
    "    Example:\n",
    "        >>> y = np.array([1, 2, 3])\n",
    "        >>> yhat = np.array([0.5, 1.5, 2.5])\n",
    "        >>> x = np.array([[1, 2], [2, 3], [3, 4]])\n",
    "        >>> Beta = np.array([0.1, 0.2])\n",
    "        >>> Var = np.eye(2)\n",
    "        >>> nu = 0.5\n",
    "        >>> result = RLSstep(y, yhat, x, Beta, Var, nu)\n",
    "    \"\"\"\n",
    "    Var = (1/nu)*(Var - ((Var.dot(x.T).dot(x).dot(Var))/(1+float(x.dot(Var).dot(x.T)))))\n",
    "    alpha = Var.dot(x.T)\n",
    "    err = y - yhat\n",
    "    Beta = Beta + alpha*err\n",
    "    yhat = x.dot(Beta)\n",
    "    return (yhat, err, Var, Beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createUpdateFunction(embeddingOrder: int, MSE_range: tuple=(None, None)) -> callable:\n",
    "    \"\"\"\n",
    "    Create an update function for stateful stream processing using Recursive Least Squares (RLS) algorithm.\n",
    "\n",
    "    Args:\n",
    "        embeddingOrder (int): The embedding order for RLS algorithm.\n",
    "        MSE_range (tuple, optional): A tuple representing the range of MSE (Mean Squared Error)\n",
    "        dates in the format (\"%Y-%m-%d\", \"%Y-%m-%d\"). During this range,\n",
    "        the MSE will be computed, considering the past as solely training. \n",
    "        Defaults to (None, None).\n",
    "\n",
    "    Returns:\n",
    "        callable: The update function that calculates and updates the state based on new values.\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "\n",
    "    Example:\n",
    "        >>> update_func = createUpdateFunction(3, (\"2022-01-01\", \"2022-12-31\"))\n",
    "    \"\"\"\n",
    "    if MSE_range != (None, None):\n",
    "        LB = datetime.datetime.strptime(MSE_range[0], \"%Y-%m-%d\")\n",
    "        HB = datetime.datetime.strptime(MSE_range[1], \"%Y-%m-%d\")\n",
    "\n",
    "    def UpdateFunction(new_values, state) -> dict:\n",
    "        \"\"\"\n",
    "        Performs an update step for stateful stream processing using Recursive Least Squares (RLS) algorithm.\n",
    "\n",
    "        Args:\n",
    "            new_values (list): The new values to be processed.\n",
    "            state (dict): The current state.\n",
    "\n",
    "        Returns:\n",
    "            dict: The updated state.\n",
    "\n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if len(new_values) > 0:\n",
    "            initial_state = state\n",
    "            OBS = sorted((initial_state[\"LastObservations\"]+new_values[0]), key= lambda x: x[5], reverse= False)\n",
    "            observations = [i[2] for i in OBS]\n",
    "            beta = initial_state[\"BetaVector\"]\n",
    "            beta.shape = (len(beta),1)\n",
    "            for i in range(embeddingOrder-1, len(observations)): \n",
    "                x = np.array(observations[i-embeddingOrder+1:i+1])\n",
    "                x = np.append(1,x)\n",
    "                x.shape= (1,len(x))\n",
    "                y = observations[i]\n",
    "                RLS_results = RLSstep(y,initial_state[\"Forecast\"], x, beta, initial_state[\"Variance\"], initial_state[\"ForgettingFactor\"] )\n",
    "                initial_state[\"Forecast\"] = RLS_results[0]\n",
    "                beta = RLS_results[3]\n",
    "                initial_state[\"Variance\"] = RLS_results[2]\n",
    "                initial_state[\"N\"] = initial_state[\"N\"] + 1\n",
    "                err = RLS_results[1]\n",
    "                if MSE_range == (None, None):\n",
    "                    \n",
    "                    initial_state[\"SSE\"] = initial_state[\"SSE\"] + err**2 if initial_state[\"N_MSE\"] != 0 else 0                     \n",
    "                    initial_state[\"MSE\"] =  initial_state[\"SSE\"]/initial_state[\"N_MSE\"] if initial_state[\"N_MSE\"] != 0 else 0 \n",
    "                    initial_state[\"N_MSE\"] = initial_state[\"N_MSE\"] + 1\n",
    "                else: \n",
    "                    if LB <= datetime.datetime.strptime(OBS[i][0], \"%Y-%m-%d\") <= HB:\n",
    "                        initial_state[\"SSE\"] = initial_state[\"SSE\"] if initial_state[\"SSE\"] != -np.inf else 0\n",
    "                        initial_state[\"SSE\"] = initial_state[\"SSE\"] + err**2\n",
    "                        initial_state[\"N_MSE\"] = initial_state[\"N_MSE\"] + 1\n",
    "                        initial_state[\"MSE\"] =  initial_state[\"SSE\"]/initial_state[\"N_MSE\"] \n",
    "                    else: \n",
    "                        initial_state[\"MSE\"] = -np.inf\n",
    "                        initial_state[\"SSE\"] = -np.inf\n",
    "                \n",
    "            initial_state[\"BetaVector\"] = beta  \n",
    "            initial_state[\"LastObservations\"] =  OBS[-embeddingOrder+1:] if embeddingOrder != 1 else []\n",
    "\n",
    "            return initial_state\n",
    "        else:\n",
    "            return state\n",
    "\n",
    "    return UpdateFunction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Order 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is similar to the last models, except the states and the update function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_ORDER =  1\n",
    "INITIALIZATION_VARIANCE = 10\n",
    "RANGE = (\"2022-04-15\",\"2023-03-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RLS = [\n",
    "    (i,{\n",
    "        \"BetaVector\": np.zeros(EMBEDDING_ORDER+1),\n",
    "        \"Variance\": np.diag(np.zeros(EMBEDDING_ORDER+1)+INITIALIZATION_VARIANCE),\n",
    "        \"ForgettingFactor\":1,\n",
    "        \"SSE\":0, \n",
    "        \"Forecast\" : 0, \n",
    "        \"N\":0, \n",
    "        \"MSE\":0,\n",
    "        \"N_MSE\":0,\n",
    "        \"LastObservations\":[]\n",
    "        }) \n",
    "    for i in sensors\n",
    "    ]\n",
    "RLSRDD = sc.parallelize(RLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_INTERVAL = 10\n",
    "\n",
    "[ssc,dstream]=getDStream(sc, BATCH_INTERVAL)\n",
    "\n",
    "dataS = dstream\\\n",
    "    .flatMap(lambda x: [*np.array(ast.literal_eval(x))])\\\n",
    "    .map(lambda x: (x[0], int(x[1]), int(x[2]), int(x[3]), x[4], int(x[5])))\\\n",
    "\n",
    "dataPerSensor = dataS\\\n",
    "    .map(lambda x: (x[4], x))\\\n",
    "    .partitionBy(NSENSORS)\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(list)\n",
    "\n",
    "updateStateFunction = createUpdateFunction(embeddingOrder= EMBEDDING_ORDER, MSE_range=RANGE)\n",
    "\n",
    "\n",
    "FutureForecasts = dataPerSensor\\\n",
    "    .updateStateByKey(updateStateFunction, initialRDD=RLSRDD)\n",
    "\n",
    "FutureForecasts.pprint()\n",
    "\n",
    "FutureForecasts.foreachRDD(lambda rdd: rdd.saveAsTextFile(f\"RLS_final_state_{EMBEDDING_ORDER}.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Order 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_ORDER =  2\n",
    "INITIALIZATION_VARIANCE = 10\n",
    "RANGE = (\"2022-04-15\",\"2023-03-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RLS = [\n",
    "    (i,{\n",
    "        \"BetaVector\": np.zeros(EMBEDDING_ORDER+1),\n",
    "        \"Variance\": np.diag(np.zeros(EMBEDDING_ORDER+1)+INITIALIZATION_VARIANCE),\n",
    "        \"ForgettingFactor\":1,\n",
    "        \"SSE\":0, \n",
    "        \"Forecast\" : 0, \n",
    "        \"N\":0, \n",
    "        \"MSE\":0,\n",
    "        \"N_MSE\":0,\n",
    "        \"LastObservations\":[]\n",
    "        }) \n",
    "    for i in sensors\n",
    "    ]\n",
    "RLSRDD = sc.parallelize(RLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_INTERVAL = 20\n",
    "\n",
    "[ssc,dstream]=getDStream(sc, BATCH_INTERVAL)\n",
    "\n",
    "dataS = dstream\\\n",
    "    .flatMap(lambda x: [*np.array(ast.literal_eval(x))])\\\n",
    "    .map(lambda x: (x[0], int(x[1]), int(x[2]), int(x[3]), x[4], int(x[5])))\\\n",
    "\n",
    "dataPerSensor = dataS\\\n",
    "    .map(lambda x: (x[4], x))\\\n",
    "    .partitionBy(NSENSORS)\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(list)\n",
    "\n",
    "updateStateFunction = createUpdateFunction(embeddingOrder= EMBEDDING_ORDER, MSE_range=RANGE)\n",
    "\n",
    "FutureForecasts = dataPerSensor\\\n",
    "    .updateStateByKey(updateStateFunction, initialRDD=RLSRDD)\n",
    "\n",
    "FutureForecasts.pprint()\n",
    "\n",
    "FutureForecasts.foreachRDD(lambda rdd: rdd.saveAsTextFile(f\"RLS_final_state_{EMBEDDING_ORDER}.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Order 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_ORDER =  3\n",
    "INITIALIZATION_VARIANCE = 10\n",
    "RANGE = (\"2022-04-15\",\"2023-03-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RLS = [\n",
    "    (i,{\n",
    "        \"BetaVector\": np.zeros(EMBEDDING_ORDER+1),\n",
    "        \"Variance\": np.diag(np.zeros(EMBEDDING_ORDER+1)+INITIALIZATION_VARIANCE),\n",
    "        \"ForgettingFactor\":1,\n",
    "        \"SSE\":0, \n",
    "        \"Forecast\" : 0, \n",
    "        \"N\":0, \n",
    "        \"MSE\":0,\n",
    "        \"N_MSE\":0,\n",
    "        \"LastObservations\":[]\n",
    "        }) \n",
    "    for i in sensors\n",
    "    ]\n",
    "RLSRDD = sc.parallelize(RLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_INTERVAL = 20\n",
    "\n",
    "[ssc,dstream]=getDStream(sc, BATCH_INTERVAL)\n",
    "\n",
    "dataS = dstream\\\n",
    "    .flatMap(lambda x: [*np.array(ast.literal_eval(x))])\\\n",
    "    .map(lambda x: (x[0], int(x[1]), int(x[2]), int(x[3]), x[4], int(x[5])))\\\n",
    "\n",
    "dataPerSensor = dataS\\\n",
    "    .map(lambda x: (x[4], x))\\\n",
    "    .partitionBy(NSENSORS)\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(list)\n",
    "\n",
    "updateStateFunction = createUpdateFunction(embeddingOrder= EMBEDDING_ORDER, MSE_range=RANGE)\n",
    "\n",
    "\n",
    "FutureForecasts = dataPerSensor\\\n",
    "    .updateStateByKey(updateStateFunction, initialRDD=RLSRDD)\n",
    "\n",
    "FutureForecasts.pprint()\n",
    "\n",
    "FutureForecasts.foreachRDD(lambda rdd: rdd.saveAsTextFile(f\"RLS_final_state_{EMBEDDING_ORDER}.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Order 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_ORDER =  4\n",
    "INITIALIZATION_VARIANCE = 10\n",
    "RANGE = (\"2022-04-15\",\"2023-03-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RLS = [\n",
    "    (i,{\n",
    "        \"BetaVector\": np.zeros(EMBEDDING_ORDER+1),\n",
    "        \"Variance\": np.diag(np.zeros(EMBEDDING_ORDER+1)+INITIALIZATION_VARIANCE),\n",
    "        \"ForgettingFactor\":1,\n",
    "        \"SSE\":0, \n",
    "        \"Forecast\" : 0, \n",
    "        \"N\":0, \n",
    "        \"MSE\":0,\n",
    "        \"N_MSE\":0,\n",
    "        \"LastObservations\":[]\n",
    "        }) \n",
    "    for i in sensors\n",
    "    ]\n",
    "RLSRDD = sc.parallelize(RLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_INTERVAL = 20\n",
    "\n",
    "[ssc,dstream]=getDStream(sc, BATCH_INTERVAL)\n",
    "\n",
    "dataS = dstream\\\n",
    "    .flatMap(lambda x: [*np.array(ast.literal_eval(x))])\\\n",
    "    .map(lambda x: (x[0], int(x[1]), int(x[2]), int(x[3]), x[4], int(x[5])))\\\n",
    "\n",
    "dataPerSensor = dataS\\\n",
    "    .map(lambda x: (x[4], x))\\\n",
    "    .partitionBy(NSENSORS)\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(list)\n",
    "\n",
    "updateStateFunction = createUpdateFunction(embeddingOrder= EMBEDDING_ORDER, MSE_range=RANGE)\n",
    "\n",
    "\n",
    "FutureForecasts = dataPerSensor\\\n",
    "    .updateStateByKey(updateStateFunction, initialRDD=RLSRDD)\n",
    "\n",
    "FutureForecasts.pprint()\n",
    "\n",
    "FutureForecasts.foreachRDD(lambda rdd: rdd.saveAsTextFile(f\"RLS_final_state_{EMBEDDING_ORDER}.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addding numerical time features\n",
    "Finally, we add the following time numerical features to the modelling:\n",
    "- Hour of the day \n",
    "- Day of the week \n",
    "- Month of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RLSstep(y: np.ndarray, yhat: np.ndarray, x: np.ndarray, Beta: np.ndarray, Var: np.ndarray, nu: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Performs a Recursive Least Squares (RLS) step for parameter estimation.\n",
    "\n",
    "    Args:\n",
    "        y (np.ndarray): The observed output values.\n",
    "        yhat (np.ndarray): The predicted output values.\n",
    "        x (np.ndarray): The input feature values.\n",
    "        Beta (np.ndarray): The estimated model parameters.\n",
    "        Var (np.ndarray): The estimated parameter covariance matrix.\n",
    "        nu (float): A forgetting factor.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing updated values of yhat (predicted output), err (prediction error),\n",
    "               Var (updated parameter covariance matrix), and Beta (updated model parameters).\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "\n",
    "    Example:\n",
    "        >>> y = np.array([1, 2, 3])\n",
    "        >>> yhat = np.array([0.5, 1.5, 2.5])\n",
    "        >>> x = np.array([[1, 2], [2, 3], [3, 4]])\n",
    "        >>> Beta = np.array([0.1, 0.2])\n",
    "        >>> Var = np.eye(2)\n",
    "        >>> nu = 0.5\n",
    "        >>> result = RLSstep(y, yhat, x, Beta, Var, nu)\n",
    "    \"\"\"\n",
    "    Var = (1/nu)*(Var - ((Var.dot(x.T).dot(x).dot(Var))/(1+float(x.dot(Var).dot(x.T)))))\n",
    "    alpha = Var.dot(x.T)\n",
    "    err = y - yhat\n",
    "    Beta = Beta + alpha*err\n",
    "    yhat = x.dot(Beta)\n",
    "    return (yhat, err, Var, Beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createUpdateFunction(embeddingOrder: int, MSE_range: tuple=(None, None)) -> callable:\n",
    "    \"\"\"\n",
    "    Create an update function for stateful stream processing using Recursive Least Squares (RLS) algorithm.\n",
    "\n",
    "    Args:\n",
    "        embeddingOrder (int): The embedding order for RLS algorithm.\n",
    "        MSE_range (tuple, optional): A tuple representing the range of MSE (Mean Squared Error)\n",
    "        dates in the format (\"%Y-%m-%d\", \"%Y-%m-%d\"). During this range,\n",
    "        the MSE will be computed, considering the past as solely training. \n",
    "        Defaults to (None, None).\n",
    "\n",
    "    Returns:\n",
    "        callable: The update function that calculates and updates the state based on new values.\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "\n",
    "    Example:\n",
    "        >>> update_func = createUpdateFunction(3, (\"2022-01-01\", \"2022-12-31\"))\n",
    "    \"\"\"\n",
    "    if MSE_range != (None, None):\n",
    "        LB = datetime.datetime.strptime(MSE_range[0], \"%Y-%m-%d\")\n",
    "        HB = datetime.datetime.strptime(MSE_range[1], \"%Y-%m-%d\")\n",
    "\n",
    "    def UpdateFunction(new_values, state) -> dict:\n",
    "        \"\"\"\n",
    "        Performs an update step for stateful stream processing using Recursive Least Squares (RLS) algorithm.\n",
    "\n",
    "        Args:\n",
    "            new_values (list): The new values to be processed.\n",
    "            state (dict): The current state.\n",
    "\n",
    "        Returns:\n",
    "            dict: The updated state.\n",
    "\n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if len(new_values) > 0:\n",
    "            initial_state = state\n",
    "            OBS = sorted((initial_state[\"LastObservations\"]+new_values[0]), key= lambda x: x[5], reverse= False)\n",
    "            observations = [i[2] for i in OBS]\n",
    "            beta = initial_state[\"BetaVector\"]\n",
    "            beta.shape = (len(beta),1)\n",
    "            for i in range(embeddingOrder-1, len(observations)): \n",
    "                x = np.array(observations[i-embeddingOrder+1:i+1]+list(OBS[i][6:]))\n",
    "                x = np.append(1,x)\n",
    "                x.shape= (1,len(x))\n",
    "                y = observations[i]\n",
    "                RLS_results = RLSstep(y,initial_state[\"Forecast\"], x, beta, initial_state[\"Variance\"], initial_state[\"ForgettingFactor\"] )\n",
    "                initial_state[\"Forecast\"] = RLS_results[0]\n",
    "                beta = RLS_results[3]\n",
    "                initial_state[\"Variance\"] = RLS_results[2]\n",
    "                initial_state[\"N\"] = initial_state[\"N\"] + 1\n",
    "                err = RLS_results[1]\n",
    "                if MSE_range == (None, None):\n",
    "                    \n",
    "                    initial_state[\"SSE\"] = initial_state[\"SSE\"] + err**2 if initial_state[\"N_MSE\"] != 0 else 0                     \n",
    "                    initial_state[\"MSE\"] =  initial_state[\"SSE\"]/initial_state[\"N_MSE\"] if initial_state[\"N_MSE\"] != 0 else 0 \n",
    "                    initial_state[\"N_MSE\"] = initial_state[\"N_MSE\"] + 1\n",
    "                else: \n",
    "                    if LB <= datetime.datetime.strptime(OBS[i][0], \"%Y-%m-%d\") <= HB:\n",
    "                        \n",
    "                        initial_state[\"SSE\"] = initial_state[\"SSE\"] if initial_state[\"SSE\"] != -np.inf else 0\n",
    "                        initial_state[\"SSE\"] = initial_state[\"SSE\"] + err**2\n",
    "                        initial_state[\"N_MSE\"] = initial_state[\"N_MSE\"] + 1\n",
    "                        initial_state[\"MSE\"] =  initial_state[\"SSE\"]/initial_state[\"N_MSE\"] \n",
    "                    else: \n",
    "                        initial_state[\"MSE\"] = -np.inf\n",
    "                        initial_state[\"SSE\"] = -np.inf\n",
    "                \n",
    "            initial_state[\"BetaVector\"] = beta  \n",
    "            initial_state[\"LastObservations\"] =  OBS[-embeddingOrder+1:] if embeddingOrder != 1 else []\n",
    "\n",
    "            return initial_state\n",
    "        else:\n",
    "            return state\n",
    "\n",
    "    return UpdateFunction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Order 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_ORDER = 1\n",
    "NUM_FEATURES = 3\n",
    "INITIALIZATION_VARIANCE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RLS = [\n",
    "    (i,{\n",
    "        \"BetaVector\": np.zeros(EMBEDDING_ORDER+NUM_FEATURES+1),\n",
    "        \"Variance\": np.diag(np.zeros(EMBEDDING_ORDER+NUM_FEATURES+1)+INITIALIZATION_VARIANCE),\n",
    "        \"ForgettingFactor\":1,\n",
    "        \"SSE\":0, \n",
    "        \"Forecast\" : 0, \n",
    "        \"N\":0, \n",
    "        \"MSE\":0,\n",
    "        \"N_MSE\":0,\n",
    "        \"LastObservations\":[]\n",
    "        }) \n",
    "    for i in sensors\n",
    "    ]\n",
    "RLSRDD = sc.parallelize(RLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_INTERVAL = 10\n",
    "\n",
    "[ssc,dstream]=getDStream(sc, BATCH_INTERVAL)\n",
    "\n",
    "# Adding the numerical features to the data\n",
    "def get_num_features(x):\n",
    "    x = (x[0], int(x[1]), int(x[2]), int(x[3]), x[4], int(x[5]))\n",
    "    date = datetime.datetime.strptime(x[0], \"%Y-%m-%d\")\n",
    "    hour =(x[1]*15)//60\n",
    "    x = x + (hour, date.weekday(), date.month)\n",
    "    return x\n",
    "\n",
    "dataS = dstream\\\n",
    "    .flatMap(lambda x: [*np.array(ast.literal_eval(x))])\\\n",
    "    .map(lambda x: get_num_features(x))\\\n",
    "\n",
    "dataPerSensor = dataS\\\n",
    "    .map(lambda x: (x[4], x))\\\n",
    "    .partitionBy(NSENSORS)\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(list)\n",
    "\n",
    "\n",
    "updateStateFunction = createUpdateFunction(embeddingOrder= EMBEDDING_ORDER)\n",
    "\n",
    "\n",
    "FutureForecasts = dataPerSensor\\\n",
    "    .updateStateByKey(updateStateFunction, initialRDD=RLSRDD)\\\n",
    "    \n",
    "\n",
    "FutureForecasts.pprint()\n",
    "\n",
    "FutureForecasts.foreachRDD(lambda rdd: rdd.saveAsTextFile(f\"RLS_num_final_state_{EMBEDDING_ORDER}.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Order 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_ORDER = 2\n",
    "NUM_FEATURES = 3\n",
    "INITIALIZATION_VARIANCE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RLS = [\n",
    "    (i,{\n",
    "        \"BetaVector\": np.zeros(EMBEDDING_ORDER+NUM_FEATURES+1),\n",
    "        \"Variance\": np.diag(np.zeros(EMBEDDING_ORDER+NUM_FEATURES+1)+INITIALIZATION_VARIANCE),\n",
    "        \"ForgettingFactor\":1,\n",
    "        \"SSE\":0, \n",
    "        \"Forecast\" : 0, \n",
    "        \"N\":0, \n",
    "        \"MSE\":0,\n",
    "        \"N_MSE\":0,\n",
    "        \"LastObservations\":[]\n",
    "        }) \n",
    "    for i in sensors\n",
    "    ]\n",
    "RLSRDD = sc.parallelize(RLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_INTERVAL = 30\n",
    "\n",
    "[ssc,dstream]=getDStream(sc, BATCH_INTERVAL)\n",
    "\n",
    "def get_num_features(x):\n",
    "    x = (x[0], int(x[1]), int(x[2]), int(x[3]), x[4], int(x[5]))\n",
    "    date = datetime.datetime.strptime(x[0], \"%Y-%m-%d\")\n",
    "    hour =(x[1]*15)//60\n",
    "    x = x + (hour, date.weekday(), date.month)\n",
    "    return x\n",
    "\n",
    "dataS = dstream\\\n",
    "    .flatMap(lambda x: [*np.array(ast.literal_eval(x))])\\\n",
    "    .map(lambda x: get_num_features(x))\\\n",
    "\n",
    "dataPerSensor = dataS\\\n",
    "    .map(lambda x: (x[4], x))\\\n",
    "    .partitionBy(NSENSORS)\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(list)\n",
    "\n",
    "\n",
    "updateStateFunction = createUpdateFunction(embeddingOrder= EMBEDDING_ORDER)\n",
    "\n",
    "\n",
    "FutureForecasts = dataPerSensor\\\n",
    "    .updateStateByKey(updateStateFunction, initialRDD=RLSRDD)\\\n",
    "    \n",
    "\n",
    "FutureForecasts.pprint()\n",
    "\n",
    "FutureForecasts.foreachRDD(lambda rdd: rdd.saveAsTextFile(f\"RLS_num_final_state_{EMBEDDING_ORDER}.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Order 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_ORDER = 3\n",
    "NUM_FEATURES = 3\n",
    "INITIALIZATION_VARIANCE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RLS = [\n",
    "    (i,{\n",
    "        \"BetaVector\": np.zeros(EMBEDDING_ORDER+NUM_FEATURES+1),\n",
    "        \"Variance\": np.diag(np.zeros(EMBEDDING_ORDER+NUM_FEATURES+1)+INITIALIZATION_VARIANCE),\n",
    "        \"ForgettingFactor\":1,\n",
    "        \"SSE\":0, \n",
    "        \"Forecast\" : 0, \n",
    "        \"N\":0, \n",
    "        \"MSE\":0,\n",
    "        \"N_MSE\":0,\n",
    "        \"LastObservations\":[]\n",
    "        }) \n",
    "    for i in sensors\n",
    "    ]\n",
    "RLSRDD = sc.parallelize(RLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_INTERVAL = 20\n",
    "\n",
    "[ssc,dstream]=getDStream(sc, BATCH_INTERVAL)\n",
    "\n",
    "def get_num_features(x):\n",
    "    x = (x[0], int(x[1]), int(x[2]), int(x[3]), x[4], int(x[5]))\n",
    "    date = datetime.datetime.strptime(x[0], \"%Y-%m-%d\")\n",
    "    hour =(x[1]*15)//60\n",
    "    x = x + (hour, date.weekday(), date.month)\n",
    "    return x\n",
    "\n",
    "dataS = dstream\\\n",
    "    .flatMap(lambda x: [*np.array(ast.literal_eval(x))])\\\n",
    "    .map(lambda x: get_num_features(x))\\\n",
    "\n",
    "dataPerSensor = dataS\\\n",
    "    .map(lambda x: (x[4], x))\\\n",
    "    .partitionBy(NSENSORS)\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(list)\n",
    "\n",
    "\n",
    "updateStateFunction = createUpdateFunction(embeddingOrder= EMBEDDING_ORDER)\n",
    "\n",
    "\n",
    "FutureForecasts = dataPerSensor\\\n",
    "    .updateStateByKey(updateStateFunction, initialRDD=RLSRDD)\\\n",
    "    \n",
    "\n",
    "FutureForecasts.pprint()\n",
    "\n",
    "FutureForecasts.foreachRDD(lambda rdd: rdd.saveAsTextFile(f\"RLS_num_final_state_{EMBEDDING_ORDER}.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Order 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_ORDER = 4\n",
    "NUM_FEATURES = 3\n",
    "INITIALIZATION_VARIANCE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RLS = [\n",
    "    (i,{\n",
    "        \"BetaVector\": np.zeros(EMBEDDING_ORDER+NUM_FEATURES+1),\n",
    "        \"Variance\": np.diag(np.zeros(EMBEDDING_ORDER+NUM_FEATURES+1)+INITIALIZATION_VARIANCE),\n",
    "        \"ForgettingFactor\":1,\n",
    "        \"SSE\":0, \n",
    "        \"Forecast\" : 0, \n",
    "        \"N\":0, \n",
    "        \"MSE\":0,\n",
    "        \"N_MSE\":0,\n",
    "        \"LastObservations\":[]\n",
    "        }) \n",
    "    for i in sensors\n",
    "    ]\n",
    "RLSRDD = sc.parallelize(RLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_INTERVAL = 20\n",
    "\n",
    "[ssc,dstream]=getDStream(sc, BATCH_INTERVAL)\n",
    "\n",
    "def get_num_features(x):\n",
    "    x = (x[0], int(x[1]), int(x[2]), int(x[3]), x[4], int(x[5]))\n",
    "    date = datetime.datetime.strptime(x[0], \"%Y-%m-%d\")\n",
    "    hour =(x[1]*15)//60\n",
    "    x = x + (hour, date.weekday(), date.month)\n",
    "    return x\n",
    "\n",
    "dataS = dstream\\\n",
    "    .flatMap(lambda x: [*np.array(ast.literal_eval(x))])\\\n",
    "    .map(lambda x: get_num_features(x))\\\n",
    "\n",
    "dataPerSensor = dataS\\\n",
    "    .map(lambda x: (x[4], x))\\\n",
    "    .partitionBy(NSENSORS)\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(list)\n",
    "\n",
    "\n",
    "updateStateFunction = createUpdateFunction(embeddingOrder= EMBEDDING_ORDER)\n",
    "\n",
    "\n",
    "FutureForecasts = dataPerSensor\\\n",
    "    .updateStateByKey(updateStateFunction, initialRDD=RLSRDD)\\\n",
    "    \n",
    "\n",
    "FutureForecasts.pprint()\n",
    "\n",
    "FutureForecasts.foreachRDD(lambda rdd: rdd.saveAsTextFile(f\"RLS_num_final_state_{EMBEDDING_ORDER}.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
